{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35317593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the class from the Python file (module)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "# from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from BinanceClient import BinanceClient\n",
    "import numpy as np\n",
    "from typing import Final\n",
    "import joblib\n",
    "from BatchFeatures import BatchFeatures\n",
    "from datetime import datetime, timedelta\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eafd5c2",
   "metadata": {},
   "source": [
    "## Load pair df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca49b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "DB_DIR = Path(\"dbs\")\n",
    "BAD_SUFFIXES = (\"UPUSDT\",\"DOWNUSDT\",\"BULLUSDT\",\"BEARUSDT\",\"3LUSDT\",\"3SUSDT\",\"5LUSDT\",\"5SUSDT\")\n",
    "\n",
    "\n",
    "def interval_slug(s: str) -> str:\n",
    "    return s.strip().replace(\" \", \"\").replace(\"/\", \"\").lower()\n",
    "\n",
    "def make_db_name(pair: str, interval: str, weeks: int) -> str:\n",
    "    return f\"{pair}_{interval_slug(interval)}_{weeks}weeks.db\"\n",
    "\n",
    "def list_downloaded_pairs(db_dir=DB_DIR):\n",
    "    \"\"\"\n",
    "    Returns list of symbols inferred from SQLite filenames in db_dir.\n",
    "    Assumes file contains a token like BTCUSDT somewhere in the name.\n",
    "    \"\"\"\n",
    "    symbols = set()\n",
    "\n",
    "    for p in Path(db_dir).glob(\"*.db\"):\n",
    "        name = p.stem  # filename without extension\n",
    "        # find the first token that looks like a Binance symbol ending with USDT\n",
    "        m = re.search(r\"([A-Z0-9]{2,20}USDT)\", name.upper())\n",
    "        if m:\n",
    "            symbols.add(m.group(1))\n",
    "\n",
    "    # filter out leveraged tokens etc\n",
    "    symbols = [s for s in symbols if s.endswith(\"USDT\") and not s.endswith(BAD_SUFFIXES)]\n",
    "    return sorted(symbols)\n",
    "    \n",
    "def load_or_fetch_pair_df(pair: str, interval: str, weeks: int) -> tuple[str, \"pd.DataFrame\"]:\n",
    "    db_name = make_db_name(pair, interval, weeks)\n",
    "    db_path = \"./db/\" + db_name\n",
    "\n",
    "    print(f\"[{pair}] DB: {db_path}\")\n",
    "\n",
    "    binance_client = BinanceClient(db_path)\n",
    "    binance_client.set_interval(interval)\n",
    "\n",
    "    df = None\n",
    "\n",
    "    if os.path.exists(db_path):\n",
    "        df = binance_client.fetch_data_from_db(pair)\n",
    "        if df is not None and not df.empty:\n",
    "            print(f\"[{pair}] Loaded {len(df):,} rows from DB.\")\n",
    "        else:\n",
    "            df = None\n",
    "\n",
    "    if df is None:\n",
    "        print(f\"[{pair}] No usable DB data found -> fetching from Binance...\")\n",
    "\n",
    "        api_secret = os.getenv(\"BINANCE_SECRET_KEY\")\n",
    "        api_key = os.getenv(\"BINANCE_API_KEY\")\n",
    "        binance_client.make(api_key, api_secret)\n",
    "\n",
    "        server_time = binance_client.get_server_time()\n",
    "        end_dt = datetime.fromtimestamp(server_time[\"serverTime\"] / 1000, tz=timezone.utc)\n",
    "        start_dt = end_dt - timedelta(weeks=weeks)\n",
    "\n",
    "        start_ms = int(start_dt.timestamp() * 1000)\n",
    "        end_ms = int(end_dt.timestamp() * 1000)\n",
    "\n",
    "        data = binance_client.fetch_data(pair, start_ms, end_ms)\n",
    "        if data is None or data.empty:\n",
    "            raise RuntimeError(f\"[{pair}] No data returned from Binance for the requested range.\")\n",
    "\n",
    "        binance_client.store_data_to_db(pair, data)\n",
    "\n",
    "        df = binance_client.fetch_data_from_db(pair)\n",
    "        if df is None or df.empty:\n",
    "            raise RuntimeError(f\"[{pair}] Data fetched/stored but DB load returned empty.\")\n",
    "\n",
    "        print(f\"[{pair}] Fetched + stored + loaded {len(df):,} rows.\")\n",
    "\n",
    "    df = df.sort_index()\n",
    "    return db_path, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae31635-347b-4493-b505-85498e0b58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def detect_volume_shocks(\n",
    "    df,\n",
    "    vol_win=144,          # 12h median\n",
    "    impulse_k=12,         # 60 min impulse\n",
    "    rvol_thresh=6.0,\n",
    "    impulse_thresh=0.04,\n",
    "    lookahead=24,\n",
    "    cooldown=12,\n",
    "):\n",
    "    d = df[[\"open\",\"high\",\"low\",\"close\",\"volume\"]].dropna().copy()\n",
    "\n",
    "    vol_med = d[\"volume\"].rolling(vol_win).median()\n",
    "    rvol = d[\"volume\"] / vol_med\n",
    "    impulse = d[\"close\"] / d[\"close\"].shift(impulse_k) - 1\n",
    "\n",
    "    events = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(d) - lookahead:\n",
    "        if rvol.iloc[i] >= rvol_thresh and impulse.iloc[i] >= impulse_thresh:\n",
    "            px0 = d[\"close\"].iloc[i]\n",
    "            future = d[\"close\"].iloc[i+1:i+1+lookahead]\n",
    "\n",
    "            events.append({\n",
    "                \"event_ts\": d.index[i],\n",
    "                \"close_event\": px0,\n",
    "                \"rvol\": float(rvol.iloc[i]),\n",
    "                \"impulse\": float(impulse.iloc[i]),\n",
    "            })\n",
    "            i += cooldown\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "# def detect_volume_events(\n",
    "#     df: pd.DataFrame,\n",
    "#     symbol: str,\n",
    "#     vol_win: int = 144,          # 12 hours on 5m\n",
    "#     impulse_k: int = 12,         # 60 min impulse\n",
    "#     rvol_thresh: float = 6.0,    # strict\n",
    "#     impulse_thresh: float = 0.04,# +4% over impulse_k\n",
    "#     lookahead: int = 24,         # 2 hours forward path\n",
    "#     cooldown: int = 12,          # avoid logging same burst repeatedly (60 min)\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Logs candidate 'flow shock' events:\n",
    "#       - RVOL spike relative to rolling median\n",
    "#       - Positive impulse over last impulse_k bars\n",
    "#     Then measures forward path stats over lookahead bars.\n",
    "#     \"\"\"\n",
    "#     d = df.copy().sort_index()\n",
    "#     d = d[[\"open\",\"high\",\"low\",\"close\",\"volume\"]].dropna()\n",
    "\n",
    "#     vol_med = d[\"volume\"].rolling(vol_win).median()\n",
    "#     rvol = d[\"volume\"] / vol_med\n",
    "#     impulse = d[\"close\"] / d[\"close\"].shift(impulse_k) - 1.0\n",
    "\n",
    "#     out = []\n",
    "#     i = 0\n",
    "#     n = len(d)\n",
    "\n",
    "#     while i < n - lookahead:\n",
    "#         if (rvol.iloc[i] >= rvol_thresh) and (impulse.iloc[i] >= impulse_thresh):\n",
    "#             px0 = float(d[\"close\"].iloc[i])\n",
    "#             ts0 = d.index[i]\n",
    "\n",
    "#             future = d[\"close\"].iloc[i+1:i+1+lookahead]\n",
    "#             fmax = float(future.max())\n",
    "#             fmin = float(future.min())\n",
    "#             max_fwd_return = fmax / px0 - 1.0\n",
    "#             max_drawdown = fmin / px0 - 1.0\n",
    "\n",
    "#             # retrace from the peak within the lookahead window\n",
    "#             # find peak time then worst after that peak\n",
    "#             peak_idx = future.values.argmax()\n",
    "#             peak_px = float(future.iloc[peak_idx])\n",
    "#             after_peak = future.iloc[peak_idx:]  # includes peak bar\n",
    "#             trough_after_peak = float(after_peak.min())\n",
    "#             max_retrace = trough_after_peak / peak_px - 1.0  # negative means retrace\n",
    "\n",
    "#             # time to max retrace (bars after event)\n",
    "#             trough_idx = after_peak.values.argmin()\n",
    "#             time_to_max_retrace_bars = int(peak_idx + trough_idx + 1)\n",
    "\n",
    "#             out.append({\n",
    "#                 \"symbol\": symbol,\n",
    "#                 \"event_ts\": ts0,\n",
    "#                 \"close_event\": px0,\n",
    "#                 \"rvol\": float(rvol.iloc[i]),\n",
    "#                 \"impulse\": float(impulse.iloc[i]),\n",
    "#                 \"max_fwd_return\": max_fwd_return,\n",
    "#                 \"max_drawdown\": max_drawdown,\n",
    "#                 \"max_retrace\": max_retrace,\n",
    "#                 \"time_to_max_retrace_bars\": time_to_max_retrace_bars,\n",
    "#             })\n",
    "\n",
    "#             i += cooldown  # skip ahead so we don't log every bar of the same burst\n",
    "#         else:\n",
    "#             i += 1\n",
    "\n",
    "#     return pd.DataFrame(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51bee877-e504-4a5c-94a7-68298b36bea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0GUSDT] DB: ./db/0GUSDT_5m_52weeks.db\n",
      "[0GUSDT] Loaded 36,249 rows from DB.\n",
      "[1INCHUSDT] DB: ./db/1INCHUSDT_5m_52weeks.db\n",
      "[1INCHUSDT] Loaded 104,832 rows from DB.\n",
      "[2ZUSDT] DB: ./db/2ZUSDT_5m_52weeks.db\n",
      "[2ZUSDT] Loaded 33,383 rows from DB.\n",
      "[AAVEUSDT] DB: ./db/AAVEUSDT_5m_52weeks.db\n",
      "[AAVEUSDT] Loaded 104,832 rows from DB.\n",
      "[ADAUSDT] DB: ./db/ADAUSDT_5m_52weeks.db\n",
      "[ADAUSDT] Loaded 104,832 rows from DB.\n",
      "[AGLDUSDT] DB: ./db/AGLDUSDT_5m_52weeks.db\n",
      "[AGLDUSDT] Loaded 104,832 rows from DB.\n",
      "[ALGOUSDT] DB: ./db/ALGOUSDT_5m_52weeks.db\n",
      "[ALGOUSDT] Loaded 104,832 rows from DB.\n",
      "[ALLOUSDT] DB: ./db/ALLOUSDT_5m_52weeks.db\n",
      "[ALLOUSDT] Loaded 21,807 rows from DB.\n",
      "[APTUSDT] DB: ./db/APTUSDT_5m_52weeks.db\n",
      "[APTUSDT] Loaded 104,832 rows from DB.\n",
      "[ARBUSDT] DB: ./db/ARBUSDT_5m_52weeks.db\n",
      "[ARBUSDT] Loaded 104,832 rows from DB.\n",
      "[ASTERUSDT] DB: ./db/ASTERUSDT_5m_52weeks.db\n",
      "[ASTERUSDT] Loaded 32,185 rows from DB.\n",
      "[ASTRUSDT] DB: ./db/ASTRUSDT_5m_52weeks.db\n",
      "[ASTRUSDT] Loaded 104,832 rows from DB.\n",
      "[AUCTIONUSDT] DB: ./db/AUCTIONUSDT_5m_52weeks.db\n",
      "[AUCTIONUSDT] Loaded 104,832 rows from DB.\n",
      "[AVAXUSDT] DB: ./db/AVAXUSDT_5m_52weeks.db\n",
      "[AVAXUSDT] Loaded 104,832 rows from DB.\n",
      "[AVNTUSDT] DB: ./db/AVNTUSDT_5m_52weeks.db\n",
      "[AVNTUSDT] Loaded 38,318 rows from DB.\n",
      "[AXLUSDT] DB: ./db/AXLUSDT_5m_52weeks.db\n",
      "[AXLUSDT] Loaded 104,832 rows from DB.\n",
      "[AXSUSDT] DB: ./db/AXSUSDT_5m_52weeks.db\n",
      "[AXSUSDT] Loaded 104,832 rows from DB.\n",
      "[BANANAUSDT] DB: ./db/BANANAUSDT_5m_52weeks.db\n",
      "[BANANAUSDT] Loaded 104,832 rows from DB.\n",
      "[BCHUSDT] DB: ./db/BCHUSDT_5m_52weeks.db\n",
      "[BCHUSDT] Loaded 104,832 rows from DB.\n",
      "[BEAMXUSDT] DB: ./db/BEAMXUSDT_5m_52weeks.db\n",
      "[BEAMXUSDT] Loaded 104,832 rows from DB.\n",
      "[BFUSDUSDT] DB: ./db/BFUSDUSDT_5m_52weeks.db\n",
      "[BFUSDUSDT] Loaded 47,718 rows from DB.\n",
      "[BIOUSDT] DB: ./db/BIOUSDT_5m_52weeks.db\n",
      "[BIOUSDT] Loaded 104,832 rows from DB.\n",
      "[BNBUSDT] DB: ./db/BNBUSDT_5m_52weeks.db\n",
      "[BNBUSDT] Loaded 104,832 rows from DB.\n",
      "[BONKUSDT] DB: ./db/BONKUSDT_5m_52weeks.db\n",
      "[BONKUSDT] Loaded 104,832 rows from DB.\n",
      "[BTCUSDT] DB: ./db/BTCUSDT_5m_52weeks.db\n",
      "[BTCUSDT] Loaded 104,832 rows from DB.\n",
      "[CHZUSDT] DB: ./db/CHZUSDT_5m_52weeks.db\n",
      "[CHZUSDT] Loaded 104,832 rows from DB.\n",
      "[CITYUSDT] DB: ./db/CITYUSDT_5m_52weeks.db\n",
      "[CITYUSDT] Loaded 104,832 rows from DB.\n",
      "[CRVUSDT] DB: ./db/CRVUSDT_5m_52weeks.db\n",
      "[CRVUSDT] Loaded 104,832 rows from DB.\n",
      "[DASHUSDT] DB: ./db/DASHUSDT_5m_52weeks.db\n",
      "[DASHUSDT] Loaded 104,832 rows from DB.\n",
      "[DODOUSDT] DB: ./db/DODOUSDT_5m_52weeks.db\n",
      "[DODOUSDT] Loaded 104,832 rows from DB.\n",
      "[DOGEUSDT] DB: ./db/DOGEUSDT_5m_52weeks.db\n",
      "[DOGEUSDT] Loaded 104,832 rows from DB.\n",
      "[DOTUSDT] DB: ./db/DOTUSDT_5m_52weeks.db\n",
      "[DOTUSDT] Loaded 104,832 rows from DB.\n",
      "[DUSKUSDT] DB: ./db/DUSKUSDT_5m_52weeks.db\n",
      "[DUSKUSDT] Loaded 104,832 rows from DB.\n",
      "[EIGENUSDT] DB: ./db/EIGENUSDT_5m_52weeks.db\n",
      "[EIGENUSDT] Loaded 104,832 rows from DB.\n",
      "[ENAUSDT] DB: ./db/ENAUSDT_5m_52weeks.db\n",
      "[ENAUSDT] Loaded 104,832 rows from DB.\n",
      "[ENSOUSDT] DB: ./db/ENSOUSDT_5m_52weeks.db\n",
      "[ENSOUSDT] Loaded 29,751 rows from DB.\n",
      "[ERAUSDT] DB: ./db/ERAUSDT_5m_52weeks.db\n",
      "[ERAUSDT] Loaded 55,480 rows from DB.\n",
      "[ETCUSDT] DB: ./db/ETCUSDT_5m_52weeks.db\n",
      "[ETCUSDT] Loaded 104,832 rows from DB.\n",
      "[ETHFIUSDT] DB: ./db/ETHFIUSDT_5m_52weeks.db\n",
      "[ETHFIUSDT] Loaded 104,832 rows from DB.\n",
      "[ETHUSDT] DB: ./db/ETHUSDT_5m_52weeks.db\n",
      "[ETHUSDT] Loaded 104,832 rows from DB.\n",
      "[EURUSDT] DB: ./db/EURUSDT_5m_52weeks.db\n",
      "[EURUSDT] Loaded 104,832 rows from DB.\n",
      "[FDUSDUSDT] DB: ./db/FDUSDUSDT_5m_52weeks.db\n",
      "[FDUSDUSDT] Loaded 104,832 rows from DB.\n",
      "[FETUSDT] DB: ./db/FETUSDT_5m_52weeks.db\n",
      "[FETUSDT] Loaded 104,832 rows from DB.\n",
      "[FILUSDT] DB: ./db/FILUSDT_5m_52weeks.db\n",
      "[FILUSDT] Loaded 104,832 rows from DB.\n",
      "[FOGOUSDT] DB: ./db/FOGOUSDT_5m_52weeks.db\n",
      "[FOGOUSDT] Loaded 2,907 rows from DB.\n",
      "[FRAXUSDT] DB: ./db/FRAXUSDT_5m_52weeks.db\n",
      "[FRAXUSDT] Loaded 3,847 rows from DB.\n",
      "[GIGGLEUSDT] DB: ./db/GIGGLEUSDT_5m_52weeks.db\n",
      "[GIGGLEUSDT] Loaded 26,793 rows from DB.\n",
      "[GPSUSDT] DB: ./db/GPSUSDT_5m_52weeks.db\n",
      "[GPSUSDT] Loaded 94,393 rows from DB.\n",
      "[HBARUSDT] DB: ./db/HBARUSDT_5m_52weeks.db\n",
      "[HBARUSDT] Loaded 104,832 rows from DB.\n",
      "[HMSTRUSDT] DB: ./db/HMSTRUSDT_5m_52weeks.db\n",
      "[HMSTRUSDT] Loaded 104,832 rows from DB.\n",
      "[ICPUSDT] DB: ./db/ICPUSDT_5m_52weeks.db\n",
      "[ICPUSDT] Loaded 104,832 rows from DB.\n",
      "[INITUSDT] DB: ./db/INITUSDT_5m_52weeks.db\n",
      "[INITUSDT] Loaded 79,728 rows from DB.\n",
      "[JSTUSDT] DB: ./db/JSTUSDT_5m_52weeks.db\n",
      "[JSTUSDT] Loaded 104,832 rows from DB.\n",
      "[JTOUSDT] DB: ./db/JTOUSDT_5m_52weeks.db\n",
      "[JTOUSDT] Loaded 104,832 rows from DB.\n",
      "[KAIAUSDT] DB: ./db/KAIAUSDT_5m_52weeks.db\n",
      "[KAIAUSDT] Loaded 104,832 rows from DB.\n",
      "[KITEUSDT] DB: ./db/KITEUSDT_5m_52weeks.db\n",
      "[KITEUSDT] Loaded 24,811 rows from DB.\n",
      "[LAYERUSDT] DB: ./db/LAYERUSDT_5m_52weeks.db\n",
      "[LAYERUSDT] Loaded 100,424 rows from DB.\n",
      "[LDOUSDT] DB: ./db/LDOUSDT_5m_52weeks.db\n",
      "[LDOUSDT] Loaded 104,832 rows from DB.\n",
      "[LINEAUSDT] DB: ./db/LINEAUSDT_5m_52weeks.db\n",
      "[LINEAUSDT] Loaded 39,633 rows from DB.\n",
      "[LINKUSDT] DB: ./db/LINKUSDT_5m_52weeks.db\n",
      "[LINKUSDT] Loaded 104,832 rows from DB.\n",
      "[LPTUSDT] DB: ./db/LPTUSDT_5m_52weeks.db\n",
      "[LPTUSDT] Loaded 104,832 rows from DB.\n",
      "[LTCUSDT] DB: ./db/LTCUSDT_5m_52weeks.db\n",
      "[LTCUSDT] Loaded 104,832 rows from DB.\n",
      "[METUSDT] DB: ./db/METUSDT_5m_52weeks.db\n",
      "[METUSDT] Loaded 21,920 rows from DB.\n",
      "[MIRAUSDT] DB: ./db/MIRAUSDT_5m_52weeks.db\n",
      "[MIRAUSDT] Loaded 35,066 rows from DB.\n",
      "[NEARUSDT] DB: ./db/NEARUSDT_5m_52weeks.db\n",
      "[NEARUSDT] Loaded 104,832 rows from DB.\n",
      "[NOMUSDT] DB: ./db/NOMUSDT_5m_52weeks.db\n",
      "[NOMUSDT] Loaded 33,508 rows from DB.\n",
      "[ONDOUSDT] DB: ./db/ONDOUSDT_5m_52weeks.db\n",
      "[ONDOUSDT] Loaded 84,127 rows from DB.\n",
      "[OPUSDT] DB: ./db/OPUSDT_5m_52weeks.db\n",
      "[OPUSDT] Loaded 104,832 rows from DB.\n",
      "[PAXGUSDT] DB: ./db/PAXGUSDT_5m_52weeks.db\n",
      "[PAXGUSDT] Loaded 104,832 rows from DB.\n",
      "[PENDLEUSDT] DB: ./db/PENDLEUSDT_5m_52weeks.db\n",
      "[PENDLEUSDT] Loaded 104,832 rows from DB.\n",
      "[PENGUUSDT] DB: ./db/PENGUUSDT_5m_52weeks.db\n",
      "[PENGUUSDT] Loaded 104,832 rows from DB.\n",
      "[PEPEUSDT] DB: ./db/PEPEUSDT_5m_52weeks.db\n",
      "[PEPEUSDT] Loaded 104,832 rows from DB.\n",
      "[PLUMEUSDT] DB: ./db/PLUMEUSDT_5m_52weeks.db\n",
      "[PLUMEUSDT] Loaded 46,274 rows from DB.\n",
      "[POLUSDT] DB: ./db/POLUSDT_5m_52weeks.db\n",
      "[POLUSDT] Loaded 104,832 rows from DB.\n",
      "[PUMPUSDT] DB: ./db/PUMPUSDT_5m_52weeks.db\n",
      "[PUMPUSDT] Loaded 39,215 rows from DB.\n",
      "[PYTHUSDT] DB: ./db/PYTHUSDT_5m_52weeks.db\n",
      "[PYTHUSDT] Loaded 104,832 rows from DB.\n",
      "[QNTUSDT] DB: ./db/QNTUSDT_5m_52weeks.db\n",
      "[QNTUSDT] Loaded 104,832 rows from DB.\n",
      "[RENDERUSDT] DB: ./db/RENDERUSDT_5m_52weeks.db\n",
      "[RENDERUSDT] Loaded 104,832 rows from DB.\n",
      "[RESOLVUSDT] DB: ./db/RESOLVUSDT_5m_52weeks.db\n",
      "[RESOLVUSDT] Loaded 65,908 rows from DB.\n",
      "[RLUSDUSDT] DB: ./db/RLUSDUSDT_5m_52weeks.db\n",
      "[RLUSDUSDT] Loaded 1,832 rows from DB.\n",
      "[ROSEUSDT] DB: ./db/ROSEUSDT_5m_52weeks.db\n",
      "[ROSEUSDT] Loaded 104,832 rows from DB.\n",
      "[RVNUSDT] DB: ./db/RVNUSDT_5m_52weeks.db\n",
      "[RVNUSDT] Loaded 104,832 rows from DB.\n",
      "[SANDUSDT] DB: ./db/SANDUSDT_5m_52weeks.db\n",
      "[SANDUSDT] Loaded 104,832 rows from DB.\n",
      "[SEIUSDT] DB: ./db/SEIUSDT_5m_52weeks.db\n",
      "[SEIUSDT] Loaded 104,832 rows from DB.\n",
      "[SENTUSDT] DB: ./db/SENTUSDT_5m_52weeks.db\n",
      "[SENTUSDT] Loaded 904 rows from DB.\n",
      "[SHIBUSDT] DB: ./db/SHIBUSDT_5m_52weeks.db\n",
      "[SHIBUSDT] Loaded 104,832 rows from DB.\n",
      "[SOLUSDT] DB: ./db/SOLUSDT_5m_52weeks.db\n",
      "[SOLUSDT] Loaded 104,832 rows from DB.\n",
      "[SOMIUSDT] DB: ./db/SOMIUSDT_5m_52weeks.db\n",
      "[SOMIUSDT] Loaded 41,782 rows from DB.\n",
      "[STGUSDT] DB: ./db/STGUSDT_5m_52weeks.db\n",
      "[STGUSDT] Loaded 104,832 rows from DB.\n",
      "[STRKUSDT] DB: ./db/STRKUSDT_5m_52weeks.db\n",
      "[STRKUSDT] Loaded 104,832 rows from DB.\n",
      "[SUIUSDT] DB: ./db/SUIUSDT_5m_52weeks.db\n",
      "[SUIUSDT] Loaded 104,832 rows from DB.\n",
      "[SUNUSDT] DB: ./db/SUNUSDT_5m_52weeks.db\n",
      "[SUNUSDT] Loaded 104,832 rows from DB.\n",
      "[TAOUSDT] DB: ./db/TAOUSDT_5m_52weeks.db\n",
      "[TAOUSDT] Loaded 104,832 rows from DB.\n",
      "[TIAUSDT] DB: ./db/TIAUSDT_5m_52weeks.db\n",
      "[TIAUSDT] Loaded 104,832 rows from DB.\n",
      "[TONUSDT] DB: ./db/TONUSDT_5m_52weeks.db\n",
      "[TONUSDT] Loaded 104,832 rows from DB.\n",
      "[TRUMPUSDT] DB: ./db/TRUMPUSDT_5m_52weeks.db\n",
      "[TRUMPUSDT] Loaded 104,832 rows from DB.\n",
      "[TRXUSDT] DB: ./db/TRXUSDT_5m_52weeks.db\n",
      "[TRXUSDT] Loaded 104,832 rows from DB.\n",
      "[TURTLEUSDT] DB: ./db/TURTLEUSDT_5m_52weeks.db\n",
      "[TURTLEUSDT] Loaded 27,547 rows from DB.\n",
      "[UNIUSDT] DB: ./db/UNIUSDT_5m_52weeks.db\n",
      "[UNIUSDT] Loaded 104,832 rows from DB.\n",
      "[USD1USDT] DB: ./db/USD1USDT_5m_52weeks.db\n",
      "[USD1USDT] Loaded 71,475 rows from DB.\n",
      "[USDCUSDT] DB: ./db/USDCUSDT_5m_52weeks.db\n",
      "[USDCUSDT] Loaded 104,832 rows from DB.\n",
      "[USDEUSDT] DB: ./db/USDEUSDT_5m_52weeks.db\n",
      "[USDEUSDT] Loaded 39,963 rows from DB.\n",
      "[VANAUSDT] DB: ./db/VANAUSDT_5m_52weeks.db\n",
      "[VANAUSDT] Loaded 104,832 rows from DB.\n",
      "[VIRTUALUSDT] DB: ./db/VIRTUALUSDT_5m_52weeks.db\n",
      "[VIRTUALUSDT] Loaded 83,432 rows from DB.\n",
      "[WBTCUSDT] DB: ./db/WBTCUSDT_5m_52weeks.db\n",
      "[WBTCUSDT] Loaded 104,832 rows from DB.\n",
      "[WCTUSDT] DB: ./db/WCTUSDT_5m_52weeks.db\n",
      "[WCTUSDT] Loaded 82,316 rows from DB.\n",
      "[WIFUSDT] DB: ./db/WIFUSDT_5m_52weeks.db\n",
      "[WIFUSDT] Loaded 104,832 rows from DB.\n",
      "[WLDUSDT] DB: ./db/WLDUSDT_5m_52weeks.db\n",
      "[WLDUSDT] Loaded 104,832 rows from DB.\n",
      "[WLFIUSDT] DB: ./db/WLFIUSDT_5m_52weeks.db\n",
      "[WLFIUSDT] Loaded 42,252 rows from DB.\n",
      "[XLMUSDT] DB: ./db/XLMUSDT_5m_52weeks.db\n",
      "[XLMUSDT] Loaded 104,832 rows from DB.\n",
      "[XPLUSDT] DB: ./db/XPLUSDT_5m_52weeks.db\n",
      "[XPLUSDT] Loaded 35,344 rows from DB.\n",
      "[XRPUSDT] DB: ./db/XRPUSDT_5m_52weeks.db\n",
      "[XRPUSDT] Loaded 104,832 rows from DB.\n",
      "[XUSDUSDT] DB: ./db/XUSDUSDT_5m_52weeks.db\n",
      "[XUSDUSDT] Loaded 90,132 rows from DB.\n",
      "[XVSUSDT] DB: ./db/XVSUSDT_5m_52weeks.db\n",
      "[XVSUSDT] Loaded 104,832 rows from DB.\n",
      "[ZECUSDT] DB: ./db/ZECUSDT_5m_52weeks.db\n",
      "[ZECUSDT] Loaded 104,832 rows from DB.\n",
      "[ZENUSDT] DB: ./db/ZENUSDT_5m_52weeks.db\n",
      "[ZENUSDT] Loaded 104,832 rows from DB.\n",
      "[ZKCUSDT] DB: ./db/ZKCUSDT_5m_52weeks.db\n",
      "[ZKCUSDT] Loaded 38,044 rows from DB.\n",
      "[ZKPUSDT] DB: ./db/ZKPUSDT_5m_52weeks.db\n",
      "[ZKPUSDT] Loaded 5,212 rows from DB.\n",
      "[ZROUSDT] DB: ./db/ZROUSDT_5m_52weeks.db\n",
      "[ZROUSDT] Loaded 104,832 rows from DB.\n"
     ]
    }
   ],
   "source": [
    "interval = \"5m\"\n",
    "weeks = 52\n",
    "\n",
    "paths = {}\n",
    "dfs = {}\n",
    "\n",
    "pairs = list_downloaded_pairs(\"db\")\n",
    "\n",
    "for sym in pairs:\n",
    "    db_path, df = load_or_fetch_pair_df(sym, interval, weeks)\n",
    "    paths[sym] = db_path\n",
    "    dfs[sym] = df\n",
    "    dfs[sym] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4bf11",
   "metadata": {},
   "source": [
    "## Load COINS, then align timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb3d43e-03d7-47f3-ad66-52e45d696ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "\n",
    "for sym, df in dfs.items():\n",
    "    ev = detect_volume_shocks(df)      # âœ… no sym passed\n",
    "    if not ev.empty:\n",
    "        ev[\"symbol\"] = sym\n",
    "        events.append(ev)\n",
    "\n",
    "events = pd.concat(events, ignore_index=True) if events else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f89400c-310b-4286-96f2-135980abdfa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6289.000000\n",
       "mean       31.162383\n",
       "std       104.710579\n",
       "min         6.000611\n",
       "25%         7.835112\n",
       "50%        11.619263\n",
       "75%        22.839509\n",
       "max      4502.710521\n",
       "Name: rvol, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(events)            # should be thousands, not tens\n",
    "events[\"impulse\"].describe()\n",
    "events[\"rvol\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6963fc3-8a04-4230-ba38-a92c9803b0af",
   "metadata": {},
   "source": [
    "## Get all Binance coin pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5e124-1a4e-46e7-ab7e-d1e16949b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "BINANCE_REST = \"https://api.binance.com\"\n",
    "\n",
    "def get_spot_usdt_symbols():\n",
    "    \"\"\"All Spot symbols that trade against USDT and are currently TRADING.\"\"\"\n",
    "    info = requests.get(f\"{BINANCE_REST}/api/v3/exchangeInfo\", timeout=20).json()\n",
    "    syms = []\n",
    "    for s in info[\"symbols\"]:\n",
    "        if s.get(\"status\") != \"TRADING\":\n",
    "            continue\n",
    "        if s.get(\"isSpotTradingAllowed\") is not True:\n",
    "            continue\n",
    "        if s.get(\"quoteAsset\") != \"USDT\":\n",
    "            continue\n",
    "\n",
    "        sym = s[\"symbol\"]\n",
    "\n",
    "        # Exclude leveraged tokens & some common non-spot-like tickers\n",
    "        bad_substrings = [\"UPUSDT\", \"DOWNUSDT\", \"BULLUSDT\", \"BEARUSDT\", \"3LUSDT\", \"3SUSDT\", \"5LUSDT\", \"5SUSDT\"]\n",
    "        if any(sym.endswith(x) for x in bad_substrings):\n",
    "            continue\n",
    "\n",
    "        syms.append(sym)\n",
    "    return sorted(set(syms))\n",
    "\n",
    "def rank_symbols_by_quote_volume(symbols):\n",
    "    \"\"\"Return DataFrame of symbols with 24h quoteVolume (USDT) sorted desc.\"\"\"\n",
    "    tickers = requests.get(f\"{BINANCE_REST}/api/v3/ticker/24hr\", timeout=20).json()\n",
    "    # Build a map for fast lookup\n",
    "    wanted = set(symbols)\n",
    "\n",
    "    rows = []\n",
    "    for t in tickers:\n",
    "        sym = t.get(\"symbol\")\n",
    "        if sym not in wanted:\n",
    "            continue\n",
    "        # quoteVolume is in quoteAsset units, here USDT\n",
    "        qv = float(t.get(\"quoteVolume\", 0.0))\n",
    "        rows.append({\n",
    "            \"symbol\": sym,\n",
    "            \"quoteVolumeUSDT_24h\": qv,\n",
    "            \"lastPrice\": float(t.get(\"lastPrice\", 0.0)),\n",
    "            \"priceChangePercent\": float(t.get(\"priceChangePercent\", 0.0)),\n",
    "            \"count\": int(t.get(\"count\", 0)),  # trade count 24h\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(\"quoteVolumeUSDT_24h\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_top_usdt_pairs(n=100, min_quote_vol_usdt=None):\n",
    "    \"\"\"Top-N by 24h quote volume; optionally filter by minimum quote volume.\"\"\"\n",
    "    syms = get_spot_usdt_symbols()\n",
    "    ranked = rank_symbols_by_quote_volume(syms)\n",
    "\n",
    "    if min_quote_vol_usdt is not None:\n",
    "        ranked = ranked[ranked[\"quoteVolumeUSDT_24h\"] >= float(min_quote_vol_usdt)].copy()\n",
    "\n",
    "    top = ranked.head(n).copy()\n",
    "    return top, ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39df23b-a2f4-413e-b913-d138d3712d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100, ranked_all = get_top_usdt_pairs(n=100)\n",
    "pairs = top100[\"symbol\"].tolist()\n",
    "\n",
    "len(pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c6504-33ce-400e-9a64-1755e81db007",
   "metadata": {},
   "source": [
    "## Detect Comporession state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10154eca-6f14-4289-a5a9-bcbbe19bd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def detect_compression_state(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    atr_short: int = 20,          # ~100 min on 5m\n",
    "    atr_long: int = 100,          # ~8 hours on 5m\n",
    "    vol_win: int = 144,           # volume median window (12 hours)\n",
    "    vol_ratio_thresh: float = 0.6,\n",
    "    rvol_thresh: float = 0.6,\n",
    "    min_duration: int = 12        # bars of sustained compression (60 min)\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects pre-shock compression state.\n",
    "\n",
    "    Returns df with added columns:\n",
    "      - atr\n",
    "      - atr_med\n",
    "      - vol_compression\n",
    "      - rvol\n",
    "      - volu_compression\n",
    "      - compression_raw\n",
    "      - compression_duration\n",
    "      - is_compressed\n",
    "    \"\"\"\n",
    "\n",
    "    d = df.copy().sort_index()\n",
    "    d = d[[\"open\",\"high\",\"low\",\"close\",\"volume\"]].dropna()\n",
    "\n",
    "    # -----------------------\n",
    "    # 1) Volatility (ATR)\n",
    "    # -----------------------\n",
    "    high = d[\"high\"]\n",
    "    low  = d[\"low\"]\n",
    "    close = d[\"close\"]\n",
    "\n",
    "    tr = pd.concat([\n",
    "        high - low,\n",
    "        (high - close.shift()).abs(),\n",
    "        (low - close.shift()).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "\n",
    "    d[\"atr\"] = tr.rolling(atr_short).mean()\n",
    "    d[\"atr_med\"] = d[\"atr\"].rolling(atr_long).median()\n",
    "\n",
    "    d[\"vol_compression\"] = d[\"atr\"] / d[\"atr_med\"]\n",
    "    d[\"is_vol_compressed\"] = d[\"vol_compression\"] <= vol_ratio_thresh\n",
    "\n",
    "    # -----------------------\n",
    "    # 2) Volume compression\n",
    "    # -----------------------\n",
    "    vol_med = d[\"volume\"].rolling(vol_win).median()\n",
    "    d[\"rvol\"] = d[\"volume\"] / vol_med\n",
    "    d[\"is_volume_compressed\"] = d[\"rvol\"] <= rvol_thresh\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Raw compression flag\n",
    "    # -----------------------\n",
    "    d[\"compression_raw\"] = (\n",
    "        d[\"is_vol_compressed\"] &\n",
    "        d[\"is_volume_compressed\"]\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # 4) Duration counter\n",
    "    # -----------------------\n",
    "    duration = np.zeros(len(d), dtype=int)\n",
    "\n",
    "    for i in range(1, len(d)):\n",
    "        if d[\"compression_raw\"].iloc[i]:\n",
    "            duration[i] = duration[i-1] + 1\n",
    "        else:\n",
    "            duration[i] = 0\n",
    "\n",
    "    d[\"compression_duration\"] = duration\n",
    "\n",
    "    # -----------------------\n",
    "    # 5) Final state\n",
    "    # -----------------------\n",
    "    d[\"is_compressed\"] = d[\"compression_duration\"] >= min_duration\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252a734-b5fb-4551-b6ac-8cb04e97daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfs[\"BTCUSDT\"]\n",
    "df_c = detect_compression_state(df)\n",
    "\n",
    "df_c[[\"vol_compression\",\"rvol\",\"compression_duration\",\"is_compressed\"]].tail(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e7c80-ac75-4944-9da3-55846269a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c[\"is_compressed\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9efd3d-f172-4940-9bfe-c07dc9ad987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compression_times(df_c):\n",
    "    \"\"\"\n",
    "    df_c = output of detect_compression_state(df)\n",
    "    Returns DatetimeIndex of bars where compression starts\n",
    "    (we take the FIRST bar of each compression block)\n",
    "    \"\"\"\n",
    "    comp = df_c[\"is_compressed\"]\n",
    "\n",
    "    # compression start = is_compressed == True AND previous == False\n",
    "    comp_start = comp & (~comp.shift(1).fillna(False))\n",
    "\n",
    "    return df_c.index[comp_start]\n",
    "\n",
    "def get_shock_times(events_sym):\n",
    "    \"\"\"\n",
    "    events_sym = events filtered to one symbol\n",
    "    Returns DatetimeIndex of shock timestamps\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(events_sym[\"event_ts\"]).sort_values()\n",
    "\n",
    "def compression_leads_shock(\n",
    "    compression_times,\n",
    "    shock_times,\n",
    "    *,\n",
    "    max_lead_minutes=120\n",
    "):\n",
    "    \"\"\"\n",
    "    For each compression start time, check whether\n",
    "    a shock occurs within max_lead_minutes AFTER it.\n",
    "    \"\"\"\n",
    "    lead_td = pd.Timedelta(minutes=max_lead_minutes)\n",
    "\n",
    "    hits = 0\n",
    "    for t in compression_times:\n",
    "        if ((shock_times > t) & (shock_times <= t + lead_td)).any():\n",
    "            hits += 1\n",
    "\n",
    "    total = len(compression_times)\n",
    "\n",
    "    return {\n",
    "        \"compression_events\": total,\n",
    "        \"hits\": hits,\n",
    "        \"hit_rate\": hits / total if total > 0 else np.nan\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2aa29-43d2-43ba-840c-26f71e270c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: ATR + linear slope\n",
    "# -----------------------------\n",
    "def _atr14(series_df: pd.DataFrame, n: int = 14) -> pd.Series:\n",
    "    h = series_df[\"high\"]\n",
    "    l = series_df[\"low\"]\n",
    "    c = series_df[\"close\"]\n",
    "    tr = pd.concat(\n",
    "        [(h - l), (h - c.shift()).abs(), (l - c.shift()).abs()],\n",
    "        axis=1\n",
    "    ).max(axis=1)\n",
    "    return tr.rolling(n).mean()\n",
    "\n",
    "def _lin_slope(y: np.ndarray) -> float:\n",
    "    \"\"\"Slope of y versus index (0..n-1). Returns 0 if too short or constant.\"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = len(y)\n",
    "    if n < 3 or np.all(~np.isfinite(y)):\n",
    "        return np.nan\n",
    "    x = np.arange(n, dtype=float)\n",
    "    y = y - np.nanmean(y)\n",
    "    x = x - x.mean()\n",
    "    denom = np.nansum(x * x)\n",
    "    return float(np.nansum(x * y) / denom) if denom > 0 else 0.0\n",
    "\n",
    "# -----------------------------\n",
    "# Candle overlap + wick/body\n",
    "# -----------------------------\n",
    "def _overlap_ratio(window: pd.DataFrame) -> float:\n",
    "    \"\"\"Mean normalized overlap between consecutive candles (0..1-ish).\"\"\"\n",
    "    if len(window) < 2:\n",
    "        return np.nan\n",
    "    h = window[\"high\"].to_numpy(dtype=float)\n",
    "    l = window[\"low\"].to_numpy(dtype=float)\n",
    "    rng = (h - l)\n",
    "    rng[rng == 0] = np.nan\n",
    "\n",
    "    # overlap between t and t-1\n",
    "    ov = np.minimum(h[1:], h[:-1]) - np.maximum(l[1:], l[:-1])\n",
    "    ov = np.maximum(ov, 0.0)\n",
    "    # normalize by current candle range\n",
    "    ov_norm = ov / rng[1:]\n",
    "    return float(np.nanmean(ov_norm))\n",
    "\n",
    "def _wick_to_body_ratio(window: pd.DataFrame) -> float:\n",
    "    \"\"\"(upper+lower wick)/body averaged over window.\"\"\"\n",
    "    o = window[\"open\"].to_numpy(dtype=float)\n",
    "    c = window[\"close\"].to_numpy(dtype=float)\n",
    "    h = window[\"high\"].to_numpy(dtype=float)\n",
    "    l = window[\"low\"].to_numpy(dtype=float)\n",
    "\n",
    "    body = np.abs(c - o)\n",
    "    body[body == 0] = np.nan\n",
    "\n",
    "    upper = h - np.maximum(o, c)\n",
    "    lower = np.minimum(o, c) - l\n",
    "    wick = upper + lower\n",
    "    return float(np.nanmean(wick / body))\n",
    "\n",
    "# -----------------------------\n",
    "# Feature extraction for one window\n",
    "# -----------------------------\n",
    "def extract_window_features(\n",
    "    d: pd.DataFrame,\n",
    "    t0: pd.Timestamp,\n",
    "    *,\n",
    "    pre_bars: int = 36,        # 180 min\n",
    "    gap_bars: int = 2,         # exclude last 10 min\n",
    "    atr_long_med_win: int = 144 # 12h baseline for median\n",
    ") -> dict | None:\n",
    "    \"\"\"\n",
    "    Compute pre-shock window features for a single event time t0.\n",
    "\n",
    "    Window is: [t0 - pre_bars, t0 - gap_bars]\n",
    "    Assumes d is indexed by datetime and has open/high/low/close/volume.\n",
    "    \"\"\"\n",
    "    if not isinstance(d.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"df must have a DatetimeIndex\")\n",
    "\n",
    "    # Ensure sorted\n",
    "    d = d.sort_index()\n",
    "\n",
    "    # Need baseline columns\n",
    "    if \"atr14\" not in d.columns:\n",
    "        d[\"atr14\"] = _atr14(d, 14)\n",
    "    if \"vol_med_12h\" not in d.columns:\n",
    "        d[\"vol_med_12h\"] = d[\"volume\"].rolling(atr_long_med_win).median()\n",
    "    if \"rvol_12h\" not in d.columns:\n",
    "        d[\"rvol_12h\"] = d[\"volume\"] / d[\"vol_med_12h\"]\n",
    "    if \"atr14_med_12h\" not in d.columns:\n",
    "        d[\"atr14_med_12h\"] = d[\"atr14\"].rolling(atr_long_med_win).median()\n",
    "\n",
    "    # Locate t0\n",
    "    if t0 not in d.index:\n",
    "        # if exact timestamp missing, align to nearest previous bar\n",
    "        loc = d.index.searchsorted(t0, side=\"right\") - 1\n",
    "        if loc < 0:\n",
    "            return None\n",
    "        t0 = d.index[loc]\n",
    "\n",
    "    end_loc = d.index.get_loc(t0) - gap_bars\n",
    "    start_loc = end_loc - pre_bars + 1\n",
    "\n",
    "    if start_loc < 0 or end_loc <= start_loc:\n",
    "        return None\n",
    "\n",
    "    w = d.iloc[start_loc:end_loc+1].copy()\n",
    "    if len(w) < max(10, pre_bars // 2):\n",
    "        return None\n",
    "\n",
    "    # Baselines at window end (most relevant \"current regime\")\n",
    "    atr_med = d[\"atr14_med_12h\"].iloc[end_loc]\n",
    "    if not np.isfinite(atr_med) or atr_med <= 0:\n",
    "        return None\n",
    "\n",
    "    # Features\n",
    "    atr_mean = float(np.nanmean(w[\"atr14\"].to_numpy(dtype=float)))\n",
    "    atr_mean_ratio = atr_mean / float(atr_med)\n",
    "\n",
    "    atr_slope = _lin_slope(w[\"atr14\"].to_numpy(dtype=float))\n",
    "    rvol_mean = float(np.nanmean(w[\"rvol_12h\"].to_numpy(dtype=float)))\n",
    "    rvol_slope = _lin_slope(w[\"rvol_12h\"].to_numpy(dtype=float))\n",
    "\n",
    "    window_high = float(np.nanmax(w[\"high\"]))\n",
    "    window_low  = float(np.nanmin(w[\"low\"]))\n",
    "    window_close = float(w[\"close\"].iloc[-1])\n",
    "    range_pct = (window_high - window_low) / window_close if window_close != 0 else np.nan\n",
    "\n",
    "    overlap_ratio = _overlap_ratio(w)\n",
    "    wick_to_body = _wick_to_body_ratio(w)\n",
    "\n",
    "    return {\n",
    "        \"t0\": t0,\n",
    "        \"win_start\": w.index[0],\n",
    "        \"win_end\": w.index[-1],\n",
    "        \"atr_mean_ratio\": atr_mean_ratio,\n",
    "        \"atr_slope\": atr_slope,\n",
    "        \"rvol_mean\": rvol_mean,\n",
    "        \"rvol_slope\": rvol_slope,\n",
    "        \"range_pct\": range_pct,\n",
    "        \"overlap_ratio\": overlap_ratio,\n",
    "        \"wick_to_body_ratio\": wick_to_body,\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Build dataset: shock vs control\n",
    "# -----------------------------\n",
    "def build_preshock_dataset(\n",
    "    dfs: dict[str, pd.DataFrame],\n",
    "    events: pd.DataFrame,\n",
    "    *,\n",
    "    pre_bars: int = 36,\n",
    "    gap_bars: int = 2,\n",
    "    control_per_event: int = 1,\n",
    "    control_lookahead_bars: int = 36,   # control must have NO shock in next 3h\n",
    "    seed: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a labeled dataset with:\n",
    "      label=1 for pre-shock windows\n",
    "      label=0 for matched control windows (same symbol, same window length)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    ev = events.copy()\n",
    "    ev[\"event_ts\"] = pd.to_datetime(ev[\"event_ts\"])\n",
    "    ev = ev.sort_values([\"symbol\", \"event_ts\"])\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Precompute shock times per symbol for fast control selection\n",
    "    shock_times_by_sym = {\n",
    "        sym: pd.to_datetime(g[\"event_ts\"]).sort_values().to_numpy(dtype=\"datetime64[ns]\")\n",
    "        for sym, g in ev.groupby(\"symbol\")\n",
    "    }\n",
    "\n",
    "    for sym, g in ev.groupby(\"symbol\"):\n",
    "        if sym not in dfs or dfs[sym].empty:\n",
    "            continue\n",
    "\n",
    "        d = dfs[sym].copy().sort_index()\n",
    "        d = d[[\"open\",\"high\",\"low\",\"close\",\"volume\"]].dropna()\n",
    "        if len(d) < (pre_bars + gap_bars + 200):\n",
    "            continue\n",
    "\n",
    "        # Make sure index is datetime\n",
    "        if not isinstance(d.index, pd.DatetimeIndex):\n",
    "            raise ValueError(f\"{sym} df index must be DatetimeIndex\")\n",
    "\n",
    "        # Precompute baseline columns once per symbol\n",
    "        d[\"atr14\"] = _atr14(d, 14)\n",
    "        d[\"vol_med_12h\"] = d[\"volume\"].rolling(144).median()\n",
    "        d[\"rvol_12h\"] = d[\"volume\"] / d[\"vol_med_12h\"]\n",
    "        d[\"atr14_med_12h\"] = d[\"atr14\"].rolling(144).median()\n",
    "\n",
    "        shock_times = shock_times_by_sym.get(sym, np.array([], dtype=\"datetime64[ns]\"))\n",
    "\n",
    "        for t0 in pd.to_datetime(g[\"event_ts\"]):\n",
    "            # --- Positive (pre-shock) ---\n",
    "            feat = extract_window_features(\n",
    "                d, t0, pre_bars=pre_bars, gap_bars=gap_bars\n",
    "            )\n",
    "            if feat is None:\n",
    "                continue\n",
    "\n",
    "            feat.update({\"symbol\": sym, \"label\": 1})\n",
    "            rows.append(feat)\n",
    "\n",
    "            # --- Controls (non-shock) ---\n",
    "            # We sample control windows that are NOT too close to any shock:\n",
    "            # specifically: no shock in [t_ctrl, t_ctrl + control_lookahead_bars]\n",
    "            # and the control window itself must exist.\n",
    "            if control_per_event <= 0:\n",
    "                continue\n",
    "\n",
    "            # Candidate range: pick end_loc such that start_loc >= 0 and end_loc within df\n",
    "            end_min = pre_bars + gap_bars  # minimal index for t0 location\n",
    "            end_max = len(d) - control_lookahead_bars - 1\n",
    "            if end_max <= end_min:\n",
    "                continue\n",
    "\n",
    "            # Convert shock_times to pandas index positions for quick exclusion\n",
    "            # We'll just exclude by time comparisons (simpler, good enough)\n",
    "            attempts = 0\n",
    "            got = 0\n",
    "            while got < control_per_event and attempts < 200:\n",
    "                attempts += 1\n",
    "                end_loc = int(rng.integers(end_min, end_max))\n",
    "                t_ctrl = d.index[end_loc]  # treat as \"event time\"\n",
    "\n",
    "                # Exclude if within pre_bars of start or near this shock t0 to avoid leakage\n",
    "                if abs((t_ctrl - feat[\"t0\"]).total_seconds()) < 6 * 3600:\n",
    "                    continue\n",
    "\n",
    "                # Check: no shock in next lookahead window\n",
    "                t1 = t_ctrl\n",
    "                t2 = t_ctrl + pd.Timedelta(minutes=5 * control_lookahead_bars)\n",
    "\n",
    "                if shock_times.size > 0:\n",
    "                    # numpy datetime64 compare\n",
    "                    t1n = np.datetime64(t1.to_datetime64())\n",
    "                    t2n = np.datetime64(t2.to_datetime64())\n",
    "                    has_future_shock = np.any((shock_times > t1n) & (shock_times <= t2n))\n",
    "                    if has_future_shock:\n",
    "                        continue\n",
    "\n",
    "                feat_c = extract_window_features(\n",
    "                    d, t_ctrl, pre_bars=pre_bars, gap_bars=gap_bars\n",
    "                )\n",
    "                if feat_c is None:\n",
    "                    continue\n",
    "\n",
    "                feat_c.update({\"symbol\": sym, \"label\": 0})\n",
    "                rows.append(feat_c)\n",
    "                got += 1\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361129a-2f7b-44be-8437-ff465d3cdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "ds = build_preshock_dataset(\n",
    "    dfs=dfs,\n",
    "    events=events,\n",
    "    pre_bars=36,            # 180 min\n",
    "    gap_bars=2,             # exclude last 10 min\n",
    "    control_per_event=1,    # start with 1:1 matching\n",
    "    control_lookahead_bars=36,  # control has no shock in next 3h\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "ds.shape, ds[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb843d-92fd-4dbc-bd60-6d9f1a2b5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any NaNs?\n",
    "ds.isna().mean().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Compare distributions quickly (shock vs control)\n",
    "cols = [\"atr_mean_ratio\",\"atr_slope\",\"rvol_mean\",\"rvol_slope\",\"range_pct\",\"overlap_ratio\",\"wick_to_body_ratio\"]\n",
    "ds.groupby(\"label\")[cols].median().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4e9f5-7dce-4d57-a986-b01669e4f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_instability_features_over_window(\n",
    "    d: pd.DataFrame,\n",
    "    *,\n",
    "    pre_bars: int = 36,\n",
    "    gap_bars: int = 2,\n",
    "    vol_med_win: int = 144\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds rolling-window (per bar) pre-shock features aligned to each bar t0,\n",
    "    computed from [t0-pre_bars, t0-gap_bars].\n",
    "    Output columns are same as dataset: atr_mean_ratio, atr_slope, rvol_mean, rvol_slope,\n",
    "    range_pct, wick_to_body_ratio.\n",
    "    \"\"\"\n",
    "    df = d.copy().sort_index()\n",
    "    df = df[[\"open\",\"high\",\"low\",\"close\",\"volume\"]].dropna()\n",
    "\n",
    "    # precompute base series\n",
    "    df[\"atr14\"] = _atr14(df, 14)\n",
    "    df[\"atr14_med_12h\"] = df[\"atr14\"].rolling(vol_med_win).median()\n",
    "    df[\"vol_med_12h\"] = df[\"volume\"].rolling(vol_med_win).median()\n",
    "    df[\"rvol_12h\"] = df[\"volume\"] / df[\"vol_med_12h\"]\n",
    "\n",
    "    # container columns\n",
    "    cols = [\"atr_mean_ratio\",\"atr_slope\",\"rvol_mean\",\"rvol_slope\",\"range_pct\",\"wick_to_body_ratio\"]\n",
    "    for c in cols:\n",
    "        df[c] = np.nan\n",
    "\n",
    "    # iterate bars (vectorizing slopes/wicks is possible later; keep correct first)\n",
    "    for end_loc in range(pre_bars + gap_bars, len(df)):\n",
    "        t0 = df.index[end_loc]\n",
    "        # window ends at end_loc-gap_bars\n",
    "        w_end = end_loc - gap_bars\n",
    "        w_start = w_end - pre_bars + 1\n",
    "        w = df.iloc[w_start:w_end+1]\n",
    "\n",
    "        atr_med = df[\"atr14_med_12h\"].iloc[w_end]\n",
    "        if not np.isfinite(atr_med) or atr_med <= 0:\n",
    "            continue\n",
    "\n",
    "        atr_mean = float(np.nanmean(w[\"atr14\"].to_numpy(dtype=float)))\n",
    "        df.at[t0, \"atr_mean_ratio\"] = atr_mean / float(atr_med)\n",
    "\n",
    "        df.at[t0, \"atr_slope\"] = _lin_slope(w[\"atr14\"].to_numpy(dtype=float))\n",
    "\n",
    "        df.at[t0, \"rvol_mean\"] = float(np.nanmean(w[\"rvol_12h\"].to_numpy(dtype=float)))\n",
    "        df.at[t0, \"rvol_slope\"] = _lin_slope(w[\"rvol_12h\"].to_numpy(dtype=float))\n",
    "\n",
    "        window_high = float(np.nanmax(w[\"high\"]))\n",
    "        window_low  = float(np.nanmin(w[\"low\"]))\n",
    "        window_close = float(w[\"close\"].iloc[-1])\n",
    "        df.at[t0, \"range_pct\"] = (window_high - window_low) / window_close if window_close else np.nan\n",
    "\n",
    "        df.at[t0, \"wick_to_body_ratio\"] = _wick_to_body_ratio(w)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def classify_instability(df_feat: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns boolean Series indexed like df_feat: True when in pre-shock instability state.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        (df_feat[\"rvol_mean\"] >= 2.0) &\n",
    "        (df_feat[\"rvol_slope\"] > 0.0) &\n",
    "        (df_feat[\"atr_mean_ratio\"] >= 1.10) &\n",
    "        (df_feat[\"wick_to_body_ratio\"] >= 1.60) &\n",
    "        (df_feat[\"range_pct\"] >= 0.035)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb632d-3f4a-4193-9898-96086a26c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym = \"BTCUSDT\"\n",
    "df_feat = compute_instability_features_over_window(dfs[sym])\n",
    "df_feat[\"is_instability\"] = classify_instability(df_feat)\n",
    "\n",
    "df_feat[\"is_instability\"].mean(), df_feat[\"is_instability\"].tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7dab9a-78bb-46bc-a645-6bb7bfa31abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold_youden(ds: pd.DataFrame, feature: str, higher_is_more_shock: bool = True):\n",
    "    x = ds[feature].to_numpy(dtype=float)\n",
    "    y = ds[\"label\"].to_numpy(dtype=int)\n",
    "\n",
    "    # candidate thresholds = percentiles (fast, robust)\n",
    "    qs = np.linspace(5, 95, 91)\n",
    "    thrs = np.percentile(x[np.isfinite(x)], qs)\n",
    "\n",
    "    best = None\n",
    "    for thr in thrs:\n",
    "        pred = (x >= thr) if higher_is_more_shock else (x <= thr)\n",
    "        pred = pred & np.isfinite(x)\n",
    "\n",
    "        tp = np.sum((pred == 1) & (y == 1))\n",
    "        fp = np.sum((pred == 1) & (y == 0))\n",
    "        tn = np.sum((pred == 0) & (y == 0))\n",
    "        fn = np.sum((pred == 0) & (y == 1))\n",
    "\n",
    "        tpr = tp / (tp + fn) if (tp + fn) else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) else 0\n",
    "        j = tpr - fpr\n",
    "\n",
    "        if best is None or j > best[\"J\"]:\n",
    "            best = {\"feature\": feature, \"thr\": float(thr), \"TPR\": tpr, \"FPR\": fpr, \"J\": j}\n",
    "\n",
    "    return best\n",
    "\n",
    "features = [\"rvol_mean\",\"rvol_slope\",\"atr_mean_ratio\",\"wick_to_body_ratio\",\"range_pct\"]\n",
    "best_list = [best_threshold_youden(ds, f, higher_is_more_shock=True) for f in features]\n",
    "pd.DataFrame(best_list).sort_values(\"J\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3e124-dbff-4c17-941f-709febf29130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_future_shock(df_feat: pd.DataFrame, events: pd.DataFrame, horizon_bars: int):\n",
    "    \"\"\"\n",
    "    For each bar in df_feat, mark whether a shock occurs within the next horizon_bars.\n",
    "    \"\"\"\n",
    "    shock_times = pd.to_datetime(events[\"event_ts\"]).values\n",
    "\n",
    "    df = df_feat.copy()\n",
    "    df[\"future_shock\"] = False\n",
    "\n",
    "    idx = df.index.values\n",
    "    for i in range(len(idx)):\n",
    "        t0 = idx[i]\n",
    "        t1 = idx[min(i + horizon_bars, len(idx) - 1)]\n",
    "        # any shock in (t0, t1]?\n",
    "        df.iloc[i, df.columns.get_loc(\"future_shock\")] = np.any(\n",
    "            (shock_times > t0) & (shock_times <= t1)\n",
    "        )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3022f-7485-42b5-a7ac-21b21aea482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pre_shock_instability_v1(df_feat: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rule-based pre-shock instability classifier.\n",
    "    Calibrated via Youden J on your dataset.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        (df_feat[\"range_pct\"] >= 0.037) &\n",
    "        (df_feat[\"rvol_slope\"] >= 0.032) &\n",
    "        (df_feat[\"rvol_mean\"] >= 2.10)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98790414-e6aa-4cbb-8c1f-7a130f23af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym = \"BTCUSDT\"\n",
    "\n",
    "df_feat = compute_instability_features_over_window(dfs[sym])\n",
    "df_feat[\"is_instability\"] = classify_pre_shock_instability_v1(df_feat)\n",
    "\n",
    "ev = events.query(\"symbol == @sym\")\n",
    "df_feat = mark_future_shock(df_feat, ev, horizon_bars=24)  # 120 min\n",
    "\n",
    "p_base = df_feat[\"future_shock\"].mean()\n",
    "p_cond = df_feat.loc[df_feat[\"is_instability\"], \"future_shock\"].mean()\n",
    "\n",
    "p_base, p_cond, p_cond / p_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e5a7fe-3228-4c99-b90f-dbc888983e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers: ATR and simple linear slope\n",
    "# ------------------------------------------------------------\n",
    "def _atr(series_df: pd.DataFrame, n: int = 14) -> pd.Series:\n",
    "    h = series_df[\"high\"]\n",
    "    l = series_df[\"low\"]\n",
    "    c = series_df[\"close\"]\n",
    "    tr = pd.concat([(h - l), (h - c.shift()).abs(), (l - c.shift()).abs()], axis=1).max(axis=1)\n",
    "    return tr.rolling(n).mean()\n",
    "\n",
    "def _lin_slope(y: np.ndarray) -> float:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = len(y)\n",
    "    if n < 3 or np.all(~np.isfinite(y)):\n",
    "        return np.nan\n",
    "    x = np.arange(n, dtype=float)\n",
    "    ym = np.nanmean(y)\n",
    "    y = y - ym\n",
    "    x = x - x.mean()\n",
    "    denom = np.nansum(x * x)\n",
    "    return float(np.nansum(x * y) / denom) if denom > 0 else 0.0\n",
    "\n",
    "def _wick_to_body_ratio(window: pd.DataFrame) -> float:\n",
    "    o = window[\"open\"].to_numpy(dtype=float)\n",
    "    c = window[\"close\"].to_numpy(dtype=float)\n",
    "    h = window[\"high\"].to_numpy(dtype=float)\n",
    "    l = window[\"low\"].to_numpy(dtype=float)\n",
    "\n",
    "    body = np.abs(c - o)\n",
    "    body[body == 0] = np.nan\n",
    "\n",
    "    upper = h - np.maximum(o, c)\n",
    "    lower = np.minimum(o, c) - l\n",
    "    wick = upper + lower\n",
    "\n",
    "    x = wick / body\n",
    "    x = x[np.isfinite(x)]\n",
    "    return float(np.mean(x)) if x.size else np.nan\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Instability classifier (your v1, based on Youden J results)\n",
    "# ------------------------------------------------------------\n",
    "def classify_pre_shock_instability_v1(df_feat: pd.DataFrame) -> pd.Series:\n",
    "    return (\n",
    "        (df_feat[\"range_pct\"] >= 0.037) &\n",
    "        (df_feat[\"rvol_slope\"] >= 0.032) &\n",
    "        (df_feat[\"rvol_mean\"] >= 2.10)\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FIX #2: Fast \"future shock within horizon\" flag (vectorized)\n",
    "# ------------------------------------------------------------\n",
    "def mark_future_shock_fast(df_feat: pd.DataFrame, events_sym: pd.DataFrame, horizon_bars: int) -> pd.DataFrame:\n",
    "    df = df_feat.copy()\n",
    "    df[\"future_shock\"] = False\n",
    "\n",
    "    if events_sym.empty:\n",
    "        return df\n",
    "\n",
    "    # Convert shock times -> indices in df.index\n",
    "    shock_times = pd.to_datetime(events_sym[\"event_ts\"])\n",
    "    shock_pos = df.index.searchsorted(shock_times)\n",
    "\n",
    "    # Keep only valid positions\n",
    "    shock_pos = shock_pos[(shock_pos >= 0) & (shock_pos < len(df))]\n",
    "    if len(shock_pos) == 0:\n",
    "        return df\n",
    "\n",
    "    shock_flag = np.zeros(len(df), dtype=int)\n",
    "    shock_flag[shock_pos] = 1\n",
    "\n",
    "    # any shock in the next horizon_bars?\n",
    "    # We shift \"backwards\" by -horizon to align \"future window starting now\"\n",
    "    future_any = (\n",
    "        pd.Series(shock_flag, index=df.index)\n",
    "        .rolling(horizon_bars, min_periods=1)\n",
    "        .sum()\n",
    "        .shift(-horizon_bars)\n",
    "        .fillna(0)\n",
    "        .gt(0)\n",
    "    )\n",
    "\n",
    "    df[\"future_shock\"] = future_any\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa13aab-367e-4ada-bd33-1ee47ebf0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Compute rolling/window features aligned to each bar t0\n",
    "# NOTE: This is still the expensive step. We cache its output.\n",
    "# ------------------------------------------------------------\n",
    "def compute_instability_features_over_window(\n",
    "    d: pd.DataFrame,\n",
    "    *,\n",
    "    pre_bars: int = 36,\n",
    "    gap_bars: int = 2,\n",
    "    vol_med_win: int = 144,   # 12h baseline\n",
    "    atr_n: int = 14\n",
    ") -> pd.DataFrame:\n",
    "    df = d.copy().sort_index()\n",
    "    df = df[[\"open\",\"high\",\"low\",\"close\",\"volume\"]].dropna()\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"df index must be DatetimeIndex\")\n",
    "\n",
    "    # base series (computed once)\n",
    "    df[\"atr14\"] = _atr(df, atr_n)\n",
    "    df[\"atr14_med_12h\"] = df[\"atr14\"].rolling(vol_med_win).median()\n",
    "    df[\"vol_med_12h\"] = df[\"volume\"].rolling(vol_med_win).median()\n",
    "    df[\"rvol_12h\"] = df[\"volume\"] / df[\"vol_med_12h\"]\n",
    "\n",
    "    # features to fill\n",
    "    cols = [\"atr_mean_ratio\",\"atr_slope\",\"rvol_mean\",\"rvol_slope\",\"range_pct\",\"wick_to_body_ratio\"]\n",
    "    for c in cols:\n",
    "        df[c] = np.nan\n",
    "\n",
    "    # Python loop (heavy). Keep correct first; optimize later if needed.\n",
    "    for end_loc in range(pre_bars + gap_bars, len(df)):\n",
    "        t0 = df.index[end_loc]\n",
    "        w_end = end_loc - gap_bars\n",
    "        w_start = w_end - pre_bars + 1\n",
    "\n",
    "        w = df.iloc[w_start:w_end+1]\n",
    "        atr_med = df[\"atr14_med_12h\"].iloc[w_end]\n",
    "        if not np.isfinite(atr_med) or atr_med <= 0:\n",
    "            continue\n",
    "\n",
    "        atr_mean = float(np.nanmean(w[\"atr14\"].to_numpy(dtype=float)))\n",
    "        df.at[t0, \"atr_mean_ratio\"] = atr_mean / float(atr_med)\n",
    "        df.at[t0, \"atr_slope\"] = _lin_slope(w[\"atr14\"].to_numpy(dtype=float))\n",
    "\n",
    "        rvol_arr = w[\"rvol_12h\"].to_numpy(dtype=float)\n",
    "        df.at[t0, \"rvol_mean\"] = float(np.nanmean(rvol_arr))\n",
    "        df.at[t0, \"rvol_slope\"] = _lin_slope(rvol_arr)\n",
    "\n",
    "        window_high = float(np.nanmax(w[\"high\"]))\n",
    "        window_low  = float(np.nanmin(w[\"low\"]))\n",
    "        window_close = float(w[\"close\"].iloc[-1])\n",
    "        df.at[t0, \"range_pct\"] = (window_high - window_low) / window_close if window_close else np.nan\n",
    "\n",
    "        df.at[t0, \"wick_to_body_ratio\"] = _wick_to_body_ratio(w)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30678dcf-f274-40aa-a997-887b7230b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# FIX #1: Cache instability once per symbol (WITH PROGRESS)\n",
    "# ============================================================\n",
    "instability_cache = {}\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for sym in tqdm(pairs, desc=\"Computing instability per symbol\"):\n",
    "    if sym not in dfs:\n",
    "        continue\n",
    "\n",
    "    df = dfs[sym]\n",
    "    if df is None or df.empty:\n",
    "        continue\n",
    "\n",
    "    df_feat = compute_instability_features_over_window(df)\n",
    "    df_feat[\"is_instability\"] = classify_pre_shock_instability_v1(df_feat)\n",
    "\n",
    "    instability_cache[sym] = df_feat[[\"is_instability\"]]\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Instability cache built in {(t1 - t0)/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504be74-7b30-4816-8664-7c404b75bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "rows = []\n",
    "horizon_bars = 24  # 120 min\n",
    "\n",
    "for sym, df_feat in tqdm(instability_cache.items(), desc=\"Cond prob (fast)\"):\n",
    "    ev_ts = events_by_sym.get(sym)\n",
    "    if ev_ts is None or len(ev_ts) == 0:\n",
    "        continue\n",
    "\n",
    "    idx = df_feat.index\n",
    "    is_inst = df_feat[\"is_instability\"].to_numpy(dtype=bool)\n",
    "    n_inst = int(is_inst.sum())\n",
    "    if n_inst < 10:\n",
    "        continue\n",
    "\n",
    "    fut = future_shock_flag(idx, ev_ts, horizon_bars=horizon_bars)\n",
    "\n",
    "    p_base = float(fut.mean())\n",
    "    p_cond = float(fut[is_inst].mean())\n",
    "\n",
    "    rows.append({\n",
    "        \"symbol\": sym,\n",
    "        \"base_prob\": p_base,\n",
    "        \"cond_prob\": p_cond,\n",
    "        \"lift\": (p_cond / p_base) if p_base > 0 else np.nan,\n",
    "        \"instability_rate\": float(is_inst.mean()),\n",
    "        \"n_instability\": n_inst,\n",
    "        \"n_shocks\": int(len(ev_ts)),\n",
    "    })\n",
    "\n",
    "prob_df = pd.DataFrame(rows).sort_values(\"lift\", ascending=False)\n",
    "prob_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d025125-f9de-4349-ba94-62b9718ba52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713985a-f281-4f6a-b4e6-603bf85bf6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8a0f36-7299-4b85-8be4-2d29c387bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_by_sym = {s: g[\"event_ts\"].values for s, g in events.groupby(\"symbol\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fab7a5-0d30-49e0-a675-3433eef53c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81a16d27bbf432eb327f0b23a60747a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Instability x4:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---- Worker must be TOP-LEVEL for multiprocessing (do not nest it) ----\n",
    "def _worker_compute_instability(sym: str, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns (sym, is_instability_df) where is_instability_df has one column: ['is_instability']\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return sym, None\n",
    "\n",
    "    df_feat = compute_instability_features_over_window(df)\n",
    "    df_feat[\"is_instability\"] = classify_pre_shock_instability_v1(df_feat)\n",
    "\n",
    "    # Return only the small piece to reduce inter-process overhead\n",
    "    out = df_feat[[\"is_instability\"]].copy()\n",
    "    return sym, out\n",
    "\n",
    "\n",
    "def build_instability_cache_parallel(dfs: dict, pairs: list, n_workers: int = 12):\n",
    "    tasks = [(sym, dfs[sym]) for sym in pairs if sym in dfs and dfs[sym] is not None and not dfs[sym].empty]\n",
    "\n",
    "    instability_cache = {}\n",
    "    t0 = time.time()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as ex:\n",
    "        futures = {ex.submit(_worker_compute_instability, sym, df): sym for sym, df in tasks}\n",
    "\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"Instability x{n_workers}\"):\n",
    "            sym = futures[fut]\n",
    "            try:\n",
    "                sym2, out = fut.result()\n",
    "                if out is not None:\n",
    "                    instability_cache[sym2] = out\n",
    "            except Exception as e:\n",
    "                print(f\"[{sym}] failed: {e}\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"Instability cache built for {len(instability_cache)}/{len(tasks)} symbols in {(t1-t0)/60:.2f} min\")\n",
    "    return instability_cache\n",
    "\n",
    "\n",
    "# Choose how many cores to use\n",
    "N_WORKERS = 4  # you said 10â€“12 is fine\n",
    "instability_cache = build_instability_cache_parallel(dfs, pairs, n_workers=N_WORKERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2eaa95-c819-4781-b90a-6ffd57a00b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def future_shock_flag(index: pd.DatetimeIndex, event_ts, horizon_bars: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns boolean array: shock occurs within next horizon_bars (aligned to index).\n",
    "    O(N) via cumulative sum, no per-bar scanning.\n",
    "    \"\"\"\n",
    "    n = len(index)\n",
    "    if event_ts is None or len(event_ts) == 0:\n",
    "        return np.zeros(n, dtype=bool)\n",
    "\n",
    "    shock_pos = index.searchsorted(pd.to_datetime(event_ts))\n",
    "    shock_pos = shock_pos[(shock_pos >= 0) & (shock_pos < n)]\n",
    "    if shock_pos.size == 0:\n",
    "        return np.zeros(n, dtype=bool)\n",
    "\n",
    "    shock_flag = np.zeros(n, dtype=np.int8)\n",
    "    shock_flag[shock_pos] = 1\n",
    "\n",
    "    c = np.cumsum(shock_flag, dtype=np.int32)\n",
    "    c_pad = np.concatenate([[0], c])  # length n+1\n",
    "\n",
    "    h = horizon_bars\n",
    "    ends = np.minimum(np.arange(n) + h, n - 1)\n",
    "    # sum in (i, i+h] = c[ends] - c[i]\n",
    "    future_counts = c_pad[ends + 1] - c_pad[np.arange(n) + 1]\n",
    "    return future_counts > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11149cc7-02b2-4e39-8657-ad9c05e92384",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df_clean = prob_df.query(\n",
    "    \"n_shocks >= 10 and n_instability >= 200\"\n",
    ").sort_values(\"lift\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cba060-8853-41d5-be4b-c1d33807c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.describe()\n",
    "prob_df[prob_df[\"lift\"] > 2.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a3abb-8de9-49a1-9750-d059d731556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.sort_values(\"instability_rate\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (binbot)",
   "language": "python",
   "name": "binbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
