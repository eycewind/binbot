{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ata/miniconda3/envs/ml-2/bin/python\n",
      "/home/ata/miniconda3/envs/ml-2/bin/jupyter\n"
     ]
    }
   ],
   "source": [
    "# Verify that we are using the correct Python (/home/ata/miniconda3/envs/ml/bin/)\n",
    "!which python\n",
    "!which jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 15:33:59.933858: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-30 15:33:59.936168: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-30 15:33:59.943334: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738233239.960072 2314351 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738233239.964953 2314351 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-30 15:33:59.986245: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 7)\n",
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")\n",
    "# Import the class from the Python file (module)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from BinanceClient import BinanceClient\n",
    "import numpy as np\n",
    "from typing import Final\n",
    "import joblib\n",
    "from BatchFeatures import BatchFeatures\n",
    "from datetime import datetime, timedelta\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch pair data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Binance client with your API credentials\n",
    "# dotenv_path = Path('.env-secret')\n",
    "# load_dotenv(dotenv_path=dotenv_path)\n",
    "api_secret = os.getenv(\"BINANCE_SECRET_KEY\")\n",
    "api_key = os.getenv(\"BINANCE_API_KEY\")\n",
    "\n",
    "# Create Binance client & initialize it\n",
    "pair = \"BTCUSDT\"\n",
    "time_delta = 12\n",
    "db_name = pair + \"_1min_\" + str(time_delta) + \"weeks.db\"\n",
    "db_name = \"BTCUSDT_1min_dry_run.db\"             # For dry run testing\n",
    "binance_client = BinanceClient(db_name)\n",
    "binance_client.set_interval(\"1m\")\n",
    "batch_feature = BatchFeatures()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fecth Data from Binance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create connection to fetch data\n",
    "binance_client.make(api_key, api_secret)\n",
    "\n",
    "# Get current server time\n",
    "server_time = binance_client.get_server_time()\n",
    "\n",
    "# Compute start and end time for the last x hours\n",
    "server_time_dt = datetime.fromtimestamp(server_time['serverTime'] / 1000, tz=datetime.timezone.utc if hasattr(datetime, 'timezone') else None)\n",
    "end_date = server_time_dt\n",
    "start_date = server_time_dt - timedelta(weeks=time_delta)\n",
    "start_date_str = int(start_date.timestamp() * 1000)  # Convert to milliseconds\n",
    "end_date_str = int(end_date.timestamp() * 1000)      # Convert to milliseconds\n",
    "\n",
    "# Fetch data\n",
    "data = binance_client.fetch_data(pair, start_date_str, end_date_str)\n",
    "binance_client.store_data_to_db(pair, data)\n",
    "\n",
    "# Check if data is fetched\n",
    "if not data.empty:\n",
    "    df = data\n",
    "else:\n",
    "    print(\"No data found!!!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch data from db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch data from db\n",
    "df = binance_client.fetch_data_from_db(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering (mind the order since some features are dependent on others)\n",
    "bf = BatchFeatures()\n",
    "\n",
    "# Must-have features\n",
    "\n",
    "# EMA: Compute for short-term and long-term spans\n",
    "bf.calculate_ema(df, spans=[5, 10, 50])  # Short-term (10), Long-term (50)\n",
    "\n",
    "# MACD: Standard MACD (12-26-9) and Fast MACD (6-13-5)\n",
    "bf.calculate_macd(df, spans={'standard': (12, 26, 9), 'fast': (6, 13, 5)})\n",
    "\n",
    "# RSI: Compute for default (14) and shorter-term (7) windows\n",
    "bf.calculate_rsi(df, windows=[7, 14])\n",
    "\n",
    "# Bollinger Bands: Compute for default 20-period with 2 standard deviations\n",
    "bf.calculate_bollinger_bands(df, window=20, num_std_dev=2)\n",
    "\n",
    "# Volume Features: Compute for default 20-period\n",
    "bf.calculate_volume_features(df, windows=[20])  # Include backward-compatible volume_ratio\n",
    "\n",
    "# Candle Features: Include optional 'candle_range' based on compatibility\n",
    "bf.calculate_candle_features(df, legacy_compatibility=True)  # Default behavior for backward compatibility\n",
    "\n",
    "\n",
    "# Optionals\n",
    "bf.calculate_sma(df)\n",
    "bf.calculate_atr(df)\n",
    "bf.calculate_moving_average_crossover(df)\n",
    "bf.calculate_historical_volatility(df)\n",
    "bf.calculate_money_flow_index(df)\n",
    "bf.calculate_roc(df)\n",
    "bf.calculate_stochastic_oscillator(df)\n",
    "bf.calculate_williams_r(df)\n",
    "\n",
    "# Low value fatures\n",
    "bf.calculate_lagged_features(df)\n",
    "bf.calculate_on_balance_volume(df)\n",
    "bf.calculate_croc(df)\n",
    "\n",
    "bf.calculate_regime(df)\n",
    "\n",
    "# drop NaNs\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def label_long_short(df, price_column=\"close\", change_threshold=0.5, window=5):\n",
    "    \"\"\"\n",
    "    Label the dataset for long/short positions based on percentage price change.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing price data.\n",
    "        price_column (str): The column name for closing price.\n",
    "        change_threshold (float): The percentage price change threshold (e.g., 0.5 for 0.5%).\n",
    "        window (int): The number of minutes (candles) over which to compute the price change.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with an added 'signal' column (1 for long, 0 for short).\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the forward percentage change in price over the given window\n",
    "    df[\"price_change\"] = (df[price_column].shift(-window) - df[price_column]) / df[price_column] * 100\n",
    "\n",
    "    # Label long (1) if price change is >= threshold, short (0) if price change is <= -threshold\n",
    "    df[\"target\"] = np.where(df[\"price_change\"] >= change_threshold, 1, \n",
    "                            np.where(df[\"price_change\"] <= -change_threshold, 0, np.nan))\n",
    "\n",
    "    # Fill NaN values with previous signal (hold the last position)\n",
    "    df[\"target\"] = df[\"target\"].ffill()\n",
    "\n",
    "    # Drop the temporary 'price_change' column\n",
    "    df.drop(columns=[\"price_change\"], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "change_threshold = 0.5  # 0.5% price change threshold\n",
    "window = 5  # 5-minute lookahead\n",
    "\n",
    "# Apply labeling\n",
    "df = label_long_short(df, change_threshold=change_threshold, window=window)\n",
    "df.dropna(subset=['target'], inplace=True)\n",
    "\n",
    "# # Check label distribution\n",
    "# label_counts = df[\"target\"].value_counts(normalize=True) * 100\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# df[\"target\"].plot(kind=\"hist\", bins=2)\n",
    "# plt.title(\"Distribution of Long/Short Signals\")\n",
    "# plt.xlabel(\"Signal (0 = Short, 1 = Long)\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test/Train Splot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the df into three parts, train, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "y.plot()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm_ema_scaler_ds.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of features to exclude\n",
    "exclude_features = [\n",
    "    'open', 'high', 'low', 'close', 'volume',  # Raw prices/volume\n",
    "    'regime', # getting string to float error so dropping for now\n",
    "    'target'  # Our label\n",
    "]\n",
    "\n",
    "features = [col for col in df.columns if col not in exclude_features]\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Use only the last 3 hours of data for training, validation, and testing\n",
    "df_recent = X.copy()  # Adjust slice as needed\n",
    "\n",
    "# Calculate split indices\n",
    "n = len(df_recent)\n",
    "train_end = int(train_ratio * n)\n",
    "val_end = train_end + int(val_ratio * n)\n",
    "\n",
    "# Perform the splits\n",
    "train_data = df_recent.iloc[:train_end]\n",
    "val_data = df_recent.iloc[train_end:val_end]\n",
    "test_data = df_recent.iloc[val_end:]\n",
    "\n",
    "# Separate features (X_*) and targets (y_*)\n",
    "X_train = train_data\n",
    "y_train = y[:train_end]\n",
    "\n",
    "X_valid = val_data\n",
    "y_valid = y[train_end:val_end]\n",
    "\n",
    "X_test = test_data\n",
    "y_test = y[val_end:]\n",
    "\n",
    "# Initialize the scaler and scale only the X_* components\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "# Transform validation and test features\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_valid_scaled = pd.DataFrame(X_valid_scaled, columns=X_valid.columns, index=X_valid.index)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, 'lstm_ema_scaler_long_short.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1.0    0.548548\n",
      "0.0    0.451452\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize the time sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length (5 hours = 300 instances for 1-minute resolution)\n",
    "seq_length = 60\n",
    "batch_size = 32*40\n",
    "\n",
    "# Create time series datasets\n",
    "tf.random.set_seed(42)  # Ensures reproducibility\n",
    "\n",
    "# Training dataset\n",
    "train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=X_train_scaled.iloc[:-seq_length].to_numpy(),  # Exclude the last 'seq_length' rows for input\n",
    "    targets=y_train.iloc[seq_length:].to_numpy(),  # Shift target by 'seq_length'\n",
    "    sequence_length=seq_length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=X_valid_scaled.iloc[:-seq_length].to_numpy(),\n",
    "    targets=y_valid.iloc[seq_length:].to_numpy(),\n",
    "    sequence_length=seq_length,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# extra code – defines a utility function we'll reuse several time\n",
    "\n",
    "def fit_and_evaluate(model, train_set, valid_set, learning_rate, epochs=500):\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=20, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "\n",
    "    # Use Adam optimizer with the specified learning rate\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile model with Binary Crossentropy loss for classification\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_set, validation_data=valid_set, epochs=epochs,\n",
    "                        callbacks=[early_stopping_cb])\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    valid_loss, valid_acc = model.evaluate(valid_set)\n",
    "    return valid_acc  # Return validation accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivar LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "input_shape = (seq_length, X_train_scaled.shape[1])  # (time_steps, features)\n",
    "model = create_lstm_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 372ms/step - loss: 0.0326 - mae: 0.1870 - val_loss: 0.0252 - val_mae: 0.1538\n",
      "Epoch 2/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 359ms/step - loss: 0.0187 - mae: 0.1344 - val_loss: 0.0248 - val_mae: 0.1536\n",
      "Epoch 3/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 362ms/step - loss: 0.0183 - mae: 0.1318 - val_loss: 0.0246 - val_mae: 0.1529\n",
      "Epoch 4/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 361ms/step - loss: 0.0179 - mae: 0.1308 - val_loss: 0.0243 - val_mae: 0.1515\n",
      "Epoch 5/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 369ms/step - loss: 0.0177 - mae: 0.1302 - val_loss: 0.0244 - val_mae: 0.1518\n",
      "Epoch 6/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 392ms/step - loss: 0.0177 - mae: 0.1299 - val_loss: 0.0243 - val_mae: 0.1516\n",
      "Epoch 7/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 382ms/step - loss: 0.0175 - mae: 0.1290 - val_loss: 0.0243 - val_mae: 0.1515\n",
      "Epoch 8/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 374ms/step - loss: 0.0175 - mae: 0.1291 - val_loss: 0.0244 - val_mae: 0.1518\n",
      "Epoch 9/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 368ms/step - loss: 0.0173 - mae: 0.1285 - val_loss: 0.0243 - val_mae: 0.1515\n",
      "Epoch 10/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 270ms/step - loss: 0.0172 - mae: 0.1284 - val_loss: 0.0243 - val_mae: 0.1519\n",
      "Epoch 11/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 247ms/step - loss: 0.0172 - mae: 0.1283 - val_loss: 0.0244 - val_mae: 0.1517\n",
      "Epoch 12/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0171 - mae: 0.1282 - val_loss: 0.0247 - val_mae: 0.1523\n",
      "Epoch 13/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0170 - mae: 0.1274 - val_loss: 0.0245 - val_mae: 0.1522\n",
      "Epoch 14/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 250ms/step - loss: 0.0170 - mae: 0.1280 - val_loss: 0.0245 - val_mae: 0.1520\n",
      "Epoch 15/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0167 - mae: 0.1267 - val_loss: 0.0246 - val_mae: 0.1518\n",
      "Epoch 16/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 248ms/step - loss: 0.0168 - mae: 0.1270 - val_loss: 0.0247 - val_mae: 0.1532\n",
      "Epoch 17/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0166 - mae: 0.1269 - val_loss: 0.0248 - val_mae: 0.1529\n",
      "Epoch 18/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0165 - mae: 0.1262 - val_loss: 0.0248 - val_mae: 0.1524\n",
      "Epoch 19/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0166 - mae: 0.1267 - val_loss: 0.0249 - val_mae: 0.1530\n",
      "Epoch 20/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0163 - mae: 0.1261 - val_loss: 0.0247 - val_mae: 0.1523\n",
      "Epoch 21/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0162 - mae: 0.1257 - val_loss: 0.0248 - val_mae: 0.1524\n",
      "Epoch 22/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 248ms/step - loss: 0.0161 - mae: 0.1260 - val_loss: 0.0251 - val_mae: 0.1539\n",
      "Epoch 23/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0161 - mae: 0.1255 - val_loss: 0.0248 - val_mae: 0.1525\n",
      "Epoch 24/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0158 - mae: 0.1250 - val_loss: 0.0255 - val_mae: 0.1541\n",
      "Epoch 25/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 250ms/step - loss: 0.0158 - mae: 0.1251 - val_loss: 0.0254 - val_mae: 0.1537\n",
      "Epoch 26/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0158 - mae: 0.1250 - val_loss: 0.0253 - val_mae: 0.1534\n",
      "Epoch 27/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 250ms/step - loss: 0.0154 - mae: 0.1237 - val_loss: 0.0251 - val_mae: 0.1533\n",
      "Epoch 28/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0153 - mae: 0.1234 - val_loss: 0.0260 - val_mae: 0.1563\n",
      "Epoch 29/500\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 249ms/step - loss: 0.0154 - mae: 0.1242 - val_loss: 0.0258 - val_mae: 0.1553\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - loss: 0.0240 - mae: 0.1473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15147694945335388"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate using the utility function\n",
    "valid_acc = fit_and_evaluate(model, train_ds, valid_ds, learning_rate=0.001, epochs=100)\n",
    "\n",
    "print(f\"Validation Accuracy: {valid_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trtained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm_ema_10candles_1min.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "joblib.dump(model, 'lstm_ema_10candles_1min__long_short.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model = joblib.load('lstm_ema_10candles_1min__long_short.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " y_test Absolute Mean: 0.1259\n",
      " y_test Standard Deviation: 0.1829\n",
      " y_test Minimum Value: -1.2396\n",
      " y_test Maximum Value: 1.4439\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each target column in y_test\n",
    "print(f\" y_test Absolute Mean: {y_test.abs().mean():.4f}\")\n",
    "print(f\" y_test Standard Deviation: {y_test.std():.4f}\")\n",
    "print(f\" y_test Minimum Value: {y_test.min():.4f}\")\n",
    "print(f\" y_test Maximum Value: {y_test.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_BALANCE=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_test_copy: 27084\n",
      "Length of y_test_copy: 27084\n",
      "Length of test_data: 27084\n",
      "seq_length: 60, nn: 10\n",
      "\u001b[1m845/845\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate sequences for LSTM input\n",
    "# seq_length = 60  # Replace with the sequence length used during training\n",
    "X_test_copy = X_test_scaled.copy()\n",
    "y_test_copy = y_test.copy()\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "nn = 10\n",
    "\n",
    "print(f\"Length of X_test_copy: {len(X_test_copy)}\")\n",
    "print(f\"Length of y_test_copy: {len(y_test_copy)}\")\n",
    "print(f\"Length of test_data: {len(test_data)}\")\n",
    "print(f\"seq_length: {seq_length}, nn: {nn}\")\n",
    "\n",
    "\n",
    "for i in range(seq_length, len(test_data)-nn):\n",
    "    X_test_list.append(X_test_copy.iloc[i-seq_length:i])  # Create sequence\n",
    "    y_test_list.append(y_test_copy.iloc[i])\n",
    "    # y_test.append( (test_data.iloc[i + nn]['close'] - test_data.iloc[i]['close']) / test_data.iloc[i]['close'] * 100)  # Price change nn candles ahead\n",
    "\n",
    "X_test_list = np.array(X_test_list)\n",
    "y_test_list = np.array(y_test_list)\n",
    "\n",
    "# Unscale predictions\n",
    "predictions = model.predict(X_test_list)  # Predictions are in scaled space\n",
    "predictions = predictions.flatten()\n",
    "\n",
    "# Prepend 'nn' NaNs to align predictions with actual changes\n",
    "predictions = np.concatenate((predictions, np.full(nn, np.nan)))\n",
    "# predictions = -1*predictions \n",
    "\n",
    "\n",
    "# Combine predictions and actual values\n",
    "results_df = X_test[seq_length:].copy()\n",
    "results_df['Predicted Change'] = predictions  # Model output: predicted change in price\n",
    "results_df['Actual Change'] = y_test   # Actual change in price (target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df['Actual Future Price'] = results_df['close']  # Actual future close price\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(12, 7))\n",
    "# results_df['Predicted Price'].plot()\n",
    "# results_df['close'].plot()\n",
    "results_df['Predicted Change'].plot(alpha=0.5)\n",
    "results_df['Actual Change'].plot(alpha=0.5)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trades(signals, prices, initial_balance=1000):\n",
    "    \"\"\"\n",
    "    Simulates trades based on signals and actual price changes.\n",
    "\n",
    "    Args:\n",
    "        actual_changes (list or pd.Series): Actual percentage changes (not predicted).\n",
    "        signals (list): List of trading signals (\"Buy\", \"Sell\", \"Hold\").\n",
    "        prices (list or pd.Series): Actual price values for the asset.\n",
    "        initial_balance (float): Starting balance of the trading account.\n",
    "\n",
    "    Returns:\n",
    "        float: Final balance or cumulative profit.\n",
    "    \"\"\"\n",
    "    balance = INITIAL_BALANCE\n",
    "    position = 0  # Tracks the number of stocks held\n",
    "    entry_price = None  # Store the price when a \"Buy\" was executed\n",
    "\n",
    "    for i, signal in enumerate(signals):\n",
    "        if signal == 1 and balance > 0:\n",
    "            # Execute a buy\n",
    "            entry_price = prices.iloc[i]  # Use .iloc for positional indexing\n",
    "            position = balance / entry_price  # Buy with all available balance\n",
    "            balance = 0  # All balance used to buy\n",
    "        elif signal == -1 and position > 0:\n",
    "            # Execute a sell\n",
    "            exit_price = prices.iloc[i]  # Use .iloc for positional indexing\n",
    "            balance = position * exit_price  # Convert position to cash\n",
    "            position = 0  # Clear position\n",
    "            entry_price = None  # Reset entry price after selling\n",
    "\n",
    "    # If there's a remaining position at the end, calculate its value\n",
    "    if position > 0 and entry_price is not None:\n",
    "        balance += position * prices.iloc[-1]  # Use .iloc for positional indexing\n",
    "\n",
    "    return balance - INITIAL_BALANCE  # Return cumulative profit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Change: -0.8875565528869629\n",
      "Max Change: 0.824345588684082\n"
     ]
    }
   ],
   "source": [
    "predict_ch_min = results_df[\"Predicted Change\"].min()\n",
    "predict_ch_max = results_df[\"Predicted Change\"].max()\n",
    "print(f'Min Change: {predict_ch_min}')\n",
    "print(f'Max Change: {predict_ch_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Buy Threshold: 0.3751, Best Sell Threshold: -0.2125565528869623\n",
      "Best Performance: 22.113288469810755\n"
     ]
    }
   ],
   "source": [
    "# Define buy and sell thresholds (e.g., absolute differences in predicted vs. actual price)\n",
    "sell_thresholds = np.arange(predict_ch_min, -0.0001, 0.005)  # Thresholds for when to \"Sell\"; Sell when price is predicted to go down\n",
    "buy_thresholds = np.arange(0.0001, predict_ch_max, 0.005)   # Thresholds for when to \"Buy\"; buy when price is predicted to go up\n",
    "\n",
    "best_buy_threshold = None\n",
    "best_sell_threshold = None\n",
    "best_performance = -np.inf\n",
    "\n",
    "ii = 0\n",
    "jj = 0\n",
    "performance = np.zeros((len(buy_thresholds), len(sell_thresholds)))\n",
    "# -1 = Sell\n",
    "# 0 = Hold\n",
    "# 1 = Buy\n",
    "for buy_th in buy_thresholds:\n",
    "    for sell_th in sell_thresholds:\n",
    "        # Generate signals\n",
    "        trading_signals = [\n",
    "            -1 if pred < sell_th else 1 if pred > buy_th else 0\n",
    "            for pred in results_df['Predicted Change']\n",
    "        ]\n",
    "\n",
    "        # Simulate trades and calculate performance\n",
    "        performance[ii, jj] = simulate_trades(\n",
    "            signals=trading_signals,\n",
    "            prices=results_df['close'],  # Use the computed predicted prices\n",
    "            initial_balance=INITIAL_BALANCE\n",
    "        )\n",
    "        # Update best thresholds if current performance is better\n",
    "        if performance[ii, jj] > best_performance:\n",
    "            best_performance = performance[ii, jj]\n",
    "            best_buy_threshold = buy_th\n",
    "            best_sell_threshold = sell_th\n",
    "        jj += 1\n",
    "    ii += 1\n",
    "    jj = 0\n",
    "print(f\"Best Buy Threshold: {best_buy_threshold}, Best Sell Threshold: {best_sell_threshold}\")\n",
    "print(f\"Best Performance: {best_performance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beck Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Balance: $1022.11\n",
      "Net Profit: $22.11\n"
     ]
    }
   ],
   "source": [
    "# Define buy & sell thrsholds\n",
    "buy_threshold = best_buy_threshold\n",
    "sell_threshold = best_sell_threshold\n",
    "# Generate signals with reversed logic\n",
    "trading_signals = [\n",
    "    -1 if pred < sell_threshold else 1 if pred > buy_threshold else 0\n",
    "    for pred in results_df['Predicted Change']\n",
    "]\n",
    "\n",
    "balance = INITIAL_BALANCE\n",
    "position = 0  # No stock initially\n",
    "trading_log = []  # To store completed trades\n",
    "\n",
    "\n",
    "\n",
    "results_df['Signal'] = trading_signals\n",
    "\n",
    "# Add 'open' and 'close' prices from the original DataFrame to results DataFrame\n",
    "# results_df['open'] = df.loc[results_df.index, 'open']\n",
    "# results_df['close'] = df.loc[results_df.index, 'close']\n",
    "\n",
    "results_df.dropna(inplace=True)\n",
    "\n",
    "# Variables to track ongoing trades\n",
    "buy_price = None\n",
    "buy_date = None\n",
    "buy_volume = None\n",
    "\n",
    "# Iterate over results_df for backtesting\n",
    "for index, row in results_df.iterrows():\n",
    "    signal = row['Signal']\n",
    "    price = row['close']  # Use 'open' price for Buy\n",
    "\n",
    "    if signal == 1 and balance > 0:\n",
    "        # Record Buy details\n",
    "        buy_price = price\n",
    "        buy_date = index\n",
    "        buy_volume = balance / price\n",
    "        position = buy_volume  # Update position\n",
    "        balance = 0  # All money is invested\n",
    "\n",
    "    elif signal == -1 and position > 0:\n",
    "        # Calculate profit/loss for the completed trade\n",
    "        sell_price = price  # Use 'close' price for Sell\n",
    "        profit_loss = (sell_price - buy_price) * buy_volume\n",
    "        balance = sell_price * buy_volume  # Update balance after selling\n",
    "        position = 0  # No stock left\n",
    "\n",
    "        # Record the completed trade in the log\n",
    "        trading_log.append({\n",
    "            \"Buy Date\": buy_date,\n",
    "            \"Buy Price\": buy_price,\n",
    "            \"Buy Volume\": buy_volume,\n",
    "            \"Sell Date\": index,\n",
    "            \"Sell Price\": sell_price,\n",
    "            \"Profit/Loss\": profit_loss\n",
    "        })\n",
    "\n",
    "        # Reset Buy details\n",
    "        buy_price = None\n",
    "        buy_date = None\n",
    "        buy_volume = None\n",
    "\n",
    "# Final portfolio value\n",
    "if position > 0:\n",
    "    final_price = results_df.iloc[-1]['Predicted Price']\n",
    "    final_profit_loss = (final_price - buy_price) * buy_volume\n",
    "    balance = final_price * buy_volume  # Update balance with remaining shares\n",
    "    trading_log.append({\n",
    "        \"Buy Date\": buy_date,\n",
    "        \"Buy Price\": buy_price,\n",
    "        \"Buy Volume\": buy_volume,\n",
    "        \"Sell Date\": results_df.index[-1],\n",
    "        \"Sell Price\": final_price,\n",
    "        \"Profit/Loss\": final_profit_loss\n",
    "    })\n",
    "\n",
    "# Convert trading log to a DataFrame for better analysis\n",
    "trading_log_df = pd.DataFrame(trading_log)\n",
    "\n",
    "# Print the final results\n",
    "print(f\"Final Balance: ${balance:.2f}\")\n",
    "print(f\"Net Profit: ${balance - INITIAL_BALANCE:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted prices\n",
    "plt.clf()\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(results_df.index, results_df['Predicted Change'], label='Predicted Change', color='blue', alpha=0.7)\n",
    "plt.plot(results_df.index, y_test/100, label='Actual Change', color='red', alpha=0.7)\n",
    "\n",
    "\n",
    "# Use trading_log_df for Buy and Sell points\n",
    "buy_signals = trading_log_df.dropna(subset=['Buy Date'])\n",
    "sell_signals = trading_log_df.dropna(subset=['Sell Date'])\n",
    "\n",
    "# Map Buy/Sell signals to values from results_df['close']\n",
    "buy_close_prices = [results_df.loc[row['Buy Date'], 'Predicted Change'] for _, row in buy_signals.iterrows()]\n",
    "sell_close_prices = [results_df.loc[row['Sell Date'], 'Predicted Change'] for _, row in sell_signals.iterrows()]\n",
    "\n",
    "# Plot Buy signals as green squares at actual 'close' prices\n",
    "plt.scatter(\n",
    "    buy_signals['Buy Date'],\n",
    "    buy_close_prices,\n",
    "    label='Buy Signal',\n",
    "    color='green',\n",
    "    marker='s',\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Plot Sell signals as red circles at actual 'close' prices\n",
    "plt.scatter(\n",
    "    sell_signals['Sell Date'],\n",
    "    sell_close_prices,\n",
    "    label='Sell Signal',\n",
    "    color='red',\n",
    "    marker='o',\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Add labels, title, legend, and grid\n",
    "plt.title(\"Trading Signals Over Predicted Prices (Using Actual Close Prices)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
